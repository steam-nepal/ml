{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import (Settings, VectorStoreIndex, SimpleDirectoryReader, PromptTemplate)\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "global query_engine\n",
    "query_engine = None\n",
    "\n",
    "def init_llm():\n",
    "    llm = Ollama(model=\"llama2\", request_timeout=300.0)\n",
    "    embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "    Settings.llm = llm\n",
    "    Settings.embed_model = embed_model\n",
    "\n",
    "\n",
    "def init_index(embed_model):\n",
    "    reader = SimpleDirectoryReader(input_dir=\"./docs\", recursive=True)\n",
    "    documents = reader.load_data()\n",
    "\n",
    "    logging.info(\"index creating with `%d` documents\", len(documents))\n",
    "\n",
    "    chroma_client = chromadb.EphemeralClient()\n",
    "    chroma_collection = chroma_client.create_collection(\"iollama\")\n",
    "\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, embed_model=embed_model)\n",
    "\n",
    "    return index\n",
    "\n",
    "\n",
    "def init_query_engine(index):\n",
    "    global query_engine\n",
    "\n",
    "    # custome prompt template\n",
    "    template = (\n",
    "        \"Imagine you are a teacher and a student is asking some questions on stem fields give as little information and direct answer as possible.\\n\\n\"\n",
    "        \"Here is some context related to the query:\\n\"\n",
    "        \"-----------------------------------------\\n\"\n",
    "        \"{context_str}\\n\"\n",
    "        \"-----------------------------------------\\n\"\n",
    "        \"Considering the above information, please respond to the following inquiry with detailed references to pythagoras theorem, \"\n",
    "        \"how and why it is used\\n\\n\"\n",
    "        \"Question: {query_str}\\n\\n\"\n",
    "        \"Answer succinctly, starting with the phrase 'According to STEAMAMIGO,' and ensure your response is understandable to someone without a prior stem background.\"\n",
    "    )\n",
    "    qa_template = PromptTemplate(template)\n",
    "\n",
    "    # build query engine with custom template\n",
    "    # text_qa_template specifies custom template\n",
    "    # similarity_top_k configure the retriever to return the top 3 most similar documents,\n",
    "    # the default value of similarity_top_k is 2\n",
    "    query_engine = index.as_query_engine(text_qa_template=qa_template, similarity_top_k=3)\n",
    "\n",
    "    return query_engine\n",
    "\n",
    "\n",
    "def chat(input_question, user):\n",
    "    global query_engine\n",
    "\n",
    "    response = query_engine.query(input_question)\n",
    "    logging.info(\"got response from llm - %s\", response)\n",
    "\n",
    "    return response.response"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
